{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center> TP3: NN and CNN with ````Pytorch``` </center></h1>\n",
    "\n",
    "The deadline for report submission is Tuesday, December 22th 2020.\n",
    "\n",
    "Note: the goal of this TP is to become familiar with 'Pytorch' and to understand how to implement Neural Nets with Pyhtor.\n",
    "\n",
    "We first list the basic function in Pythor and consider a very simple example to understand how Grandient Descent can be implemented. Then we illustrate how set the architecture of neural nets and run it on MNIST dataset. Lastly, we provide an implementation of CNN.\n",
    "\n",
    "As a homework, we propose you implement logistic regression as a neural net and to also to add dropout in CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch operates with tensors instead of numpy arrays. Almost everything you can do with numpy arrays can be acomplished with pytorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9021, 0.7454, 0.7590],\n",
      "        [0.3566, 0.8614, 0.5295],\n",
      "        [0.6487, 0.9060, 0.1080]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(3, 3) # random tensor of size 3 by 3\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the result of:\n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      " +\n",
      " tensor([[4., 4., 4.],\n",
      "        [4., 4., 4.],\n",
      "        [4., 4., 4.]]) \n",
      " = \n",
      " tensor([[5., 5., 5.],\n",
      "        [5., 5., 5.],\n",
      "        [5., 5., 5.]])\n"
     ]
    }
   ],
   "source": [
    "# We can operate with pytorch tensors pretty much in the same manner as with numpy arrays\n",
    "x = torch.ones(3,3)\n",
    "y = torch.ones(3,3) * 4\n",
    "z = x + y\n",
    "print(f'This is the result of:\\n {x}\\n +\\n {y} \\n = \\n {z}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From \n",
      " tensor([[5., 5., 5.],\n",
      "        [5., 5., 5.],\n",
      "        [5., 5., 5.]]) we can look at the last row and 2 columns \n",
      " tensor([5., 5.])\n"
     ]
    }
   ],
   "source": [
    "# again we can operate with tensor indexing as if it was a numpy one\n",
    "\n",
    "x = torch.ones(3,3) * 5\n",
    "y = x[-1, :2]\n",
    "print(f'From \\n {x} we can look at the last row and 2 columns \\n {y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you know, a lot of ML algorithms can be stated as optimization problems.\n",
    "Let us consider a toy example: imagine that our data is $x = (1, \\ldots, 1)^\\top \\in \\mathbb{R}^{5}$ is a vector composed of all ones and a label $y = 1$. We would like to find a weight vector $w \\in \\mathbb{R}^{5}$ such that the loss function $L(w) = (y - x^\\top w)^2$ is minimized.\n",
    "\n",
    "Of course, this is a simple least squares on a single observation $(x, y)$ and we can compute the result analytically. But it is a good example to understand what pytorch has to offer.\n",
    "\n",
    "If we are too lazy to compute the analytic expression, we can run the Gradient Descent, which starts from $w_0 = (0, \\ldots, 0)^\\top$ and proceeds as\n",
    "\n",
    "$$w_k = w_{k - 1} - \\eta \\nabla L(w_{k - 1}).$$\n",
    "\n",
    "So the only thing that we need to know is the gradient of the loss function $L$ evaluated at the point $w_{k - 1}$.\n",
    "Here how it is done in pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.]])\n"
     ]
    }
   ],
   "source": [
    "# Input data\n",
    "y = torch.ones(1, 1)\n",
    "x = torch.ones(1, 5)\n",
    "\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these tensors during the backward pass.\n",
    "w = torch.zeros(5, 1, requires_grad=True) # setting w_0 = (0, ..., 0)^T\n",
    "\n",
    "y_pred = x.mm(w) # inner product of w and x \n",
    "\n",
    "loss = (y - y_pred).pow(2) # squared loss\n",
    "\n",
    "\n",
    "# Use autograd to compute the backward pass. This call will compute the\n",
    "# gradient of loss with respect to all tensors with requires_grad=True.\n",
    "# After this call w.grad will be a tensor holding the gradient\n",
    "# of the loss with respect to w.\n",
    "loss.backward()\n",
    "\n",
    "print(w.grad) # Print the gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question.** Assuming that $w_0 = (0, \\ldots, 0)^\\top$ compute on paper $\\nabla L(w_0)$. Do not include the answer to this question into the report. Just make sure you understant what is going on here.\n",
    "\n",
    "Once you made sure that ```w.grad``` indeed stores the value of $\\nabla L(w_0)$. We can implement the Gradient Descent algorithm with only few lines of code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10/2, Current loss: 0.0\n",
      "Final result: tensor([[ 0.6292],\n",
      "        [-1.5410],\n",
      "        [ 0.5573],\n",
      "        [-0.2596],\n",
      "        [ 1.6140]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Input data\n",
    "y = torch.ones(1, 1)\n",
    "x = torch.ones(1, 5)\n",
    "\n",
    "w = torch.zeros(5, 1, requires_grad=True) # Initialization: w_0 = (0, ..., 0)^T\n",
    "w = torch.randn(5, 1, requires_grad=True) # Initialization: w_0 = (0, ..., 0)^T\n",
    "\n",
    "lr = .1 # Learning rate a.k.a. the step size\n",
    "max_iter = 2\n",
    "\n",
    "for k in range(10):\n",
    "    loss = (y - x.mm(w)).pow(2) # forward pass\n",
    "    \n",
    "        \n",
    "    loss.backward() # the backward pass\n",
    "    \n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this\n",
    "    # in autograd.\n",
    "    with torch.no_grad():\n",
    "        w -= lr * w.grad # gradient step\n",
    "        w.grad.zero_() # after performing operation with gradient we need to erase it\n",
    "    \n",
    "    if k % 10 == 9:\n",
    "        print(f'Iteration {k + 1}/{max_iter}, Current loss: {loss.item()}')\n",
    "        \n",
    "print(f'Final result: {w}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5032],\n",
       "        [-1.7325],\n",
       "        [-1.2241],\n",
       "        [ 0.5616],\n",
       "        [ 0.0143]], requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.randn(5, 1, requires_grad=True)\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Solve the problem $\\min_{w \\in \\mathbb{R}^5}\\, (1 - x^\\top w)^2$ with $x = (1, \\ldots, 1)^\\top \\in \\mathbb{R}^5$ analytically and compare to the result of the Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10/150, Current loss: 0.150094673037529\n",
      "Iteration 20/150, Current loss: 0.018248017877340317\n",
      "Iteration 30/150, Current loss: 0.002218528650701046\n",
      "Iteration 40/150, Current loss: 0.00026972233899869025\n",
      "Iteration 50/150, Current loss: 3.279230440966785e-05\n",
      "Iteration 60/150, Current loss: 3.986556748714065e-06\n",
      "Iteration 70/150, Current loss: 4.846697265747935e-07\n",
      "Iteration 80/150, Current loss: 5.8908199207508005e-08\n",
      "Iteration 90/150, Current loss: 7.173785121494802e-09\n",
      "Iteration 100/150, Current loss: 8.74024408403784e-10\n",
      "Iteration 110/150, Current loss: 1.0756195933936397e-10\n",
      "Iteration 120/150, Current loss: 1.3219647598816664e-11\n",
      "Iteration 130/150, Current loss: 1.566746732351021e-12\n",
      "Iteration 140/150, Current loss: 1.7408297026122455e-13\n",
      "Iteration 150/150, Current loss: 1.2789769243681803e-13\n",
      "Final result: tensor([[0.2000],\n",
      "        [0.2000],\n",
      "        [0.2000],\n",
      "        [0.2000],\n",
      "        [0.2000]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(1, 5)\n",
    "\n",
    "w = torch.zeros(5, 1, requires_grad=True) # Initialization: w_0 = (0, ..., 0)^T\n",
    "\n",
    "lr = .01 # Learning rate a.k.a. the step size\n",
    "max_iter = 150\n",
    "\n",
    "for k in range(max_iter):\n",
    "    loss = (1 - x.mm(w)).pow(2) # forward pass\n",
    "    \n",
    "        \n",
    "    loss.backward() # the backward pass\n",
    "    \n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this\n",
    "    # in autograd.\n",
    "    with torch.no_grad():\n",
    "        w -= lr * w.grad # gradient step\n",
    "        w.grad.zero_() # after performing operation with gradient we need to erase it\n",
    "    \n",
    "    if k % 10 == 9:\n",
    "        print(f'Iteration {k + 1}/{max_iter}, Current loss: {loss.item()}')\n",
    "        \n",
    "print(f'Final result: {w}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Recalling the theory of numerical optimization, what is the learning rate ```lr``` that we need to set to ensure the fastest convergence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Question:** Explain the connection of ```loss.backward()``` and the backpropagation for feedforward neural nets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` torch.autograd ```  provides classes and functions implementing automatic differentiation of arbitrary scalar valued functions. It requires minimal changes to the existing code - you only need to declare Tensor s for which gradients should be computed with the requires_grad=True keyword. As of now, we only support autograd for floating point Tensor types ( half, float, double and bfloat16) and complex Tensor types (cfloat, cdouble).\n",
    "\n",
    "loss.backward() computes dloss/dx for every parameter x which has requires_grad=True. These are accumulated into x.grad for every parameter x. In pseudo-code:\n",
    "\n",
    "```x.grad += dloss/dx```\n",
    "optimizer.step updates the value of x using the gradient x.grad. For example, the SGD optimizer performs:\n",
    "\n",
    "```x += -lr * x.grad```\n",
    "optimizer.zero_grad() clears x.grad for every parameter x in the optimizer. It’s important to call this before loss.backward(), otherwise you’ll accumulate the gradients from multiple passes.\n",
    "\n",
    "If you have multiple losses (loss1, loss2) you can sum them and then call backwards once:\n",
    "\n",
    "```loss3 = loss1 + loss2\n",
    "loss3.backward()```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi layer perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we will build our neural net. Recall that MNIST is composed of images of size $28 \\times 28$, hence the dimension of the input is $784$. We have $10$ classes, so the dimension of the output is $10$.\n",
    "\n",
    "In between we will insert $2$ hidden layers and use ReLU as our non-linearity (activation function).\n",
    "The first hidden layer is composed of $128$ neurons and the second one of $64$ neurons.\n",
    "\n",
    "We will not use GPU nor we will consider complicated neural nets in this TP. The goal is to introduce you to the basics without going into too complicated architechtures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleFeedForward(nn.Module):\n",
    "    def __init__(self, input_size=784, hidden_sizes=[128, 64],\n",
    "                 output_size=10):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_sizes[0]), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_sizes[1], output_size)\n",
    "        )\n",
    "             \n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, input_size)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we defined our neural net we need to train it.\n",
    "The training is going to be performed via Stochastic Gradient Descent evaluated on a mini batch of the data.\n",
    "That is, on the foward stage we will use not a single data point but several ones. In this case we set the size of mini batch equal to $32$.\n",
    "\n",
    "Actually, size of the mini batch, learning rate sizes of hidden layers are all considered as hyperparameters that can be finely tuned (some people even tune random seed, which is absolutely ridiculous). We will not talk about the hypeparameter tuning in this TP, to learn more have a look at https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html .\n",
    "\n",
    "\n",
    "**Important:** We do not require you to perform complicated hyperparameter tuning. This part is beyond the course. However, it is important that you can clearly write an architecture of a neural net that you consider.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training consists of gradient steps over mini batch of data\n",
    "\n",
    "def train(model, trainloader, loss, optimizer, epoch, num_epochs):\n",
    "    # We enter train mode. This is useless for the linear model\n",
    "    # but is important for layers such as dropout, batchnorm, ...\n",
    "    model.train()    \n",
    "    loop = tqdm(trainloader)\n",
    "    loop.set_description(f'Training Epoch [{epoch + 1}/{num_epochs}]')\n",
    "    \n",
    "    # We iterate over the mini batches of our data\n",
    "    ## Changed the tqdm module\n",
    "    for inputs, targets in loop:\n",
    "        # Erase any previously stored gradient\n",
    "        optimizer.zero_grad()               \n",
    "        outputs = model(inputs) # Forwards stage (prediction with current weights)\n",
    "        loss = criterion(outputs, targets) # loss evaluation        \n",
    "        loss.backward() # Back propagation (evaluate gradients)                 \n",
    "        # Making gradient step on the batch (this function takes care of the gradient step for us)\n",
    "        optimizer.step() \n",
    "        \n",
    "        \n",
    "def validation(model, valloader, loss):\n",
    "    # Do not compute gradient, since we do not need it for validation step\n",
    "    with torch.no_grad():\n",
    "        # We enter evaluation mode.\n",
    "        model.eval() # Sets the module to eval mode. this has effect on Droupout,Batchnorm ...         \n",
    "        total = 0 # keep track of currently used samples\n",
    "        running_loss = 0.0 # accumulated loss without averagind\n",
    "        accuracy = 0.0 # accumulated accuracy without averagind (number of correct predictions)\n",
    "        \n",
    "        loop = tqdm(valloader) # This is for the progress bar\n",
    "        loop.set_description('Validation in progress')\n",
    "        \n",
    "        \n",
    "        # We again iterate over the batches of validation data. batch_size does not play any role here\n",
    "        for inputs, targets in loop:\n",
    "            # Run samples through our net\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Total number of used samples\n",
    "            total += inputs.shape[0]\n",
    "\n",
    "            # Multiply loss by the batch size to erase averaging on the batch\n",
    "            running_loss += inputs.shape[0] * loss(outputs, targets).item()\n",
    "            \n",
    "            # how many correct predictions\n",
    "            accuracy += (outputs.argmax(dim=1) == targets).sum().item()\n",
    "            \n",
    "            # set nice progress meassage\n",
    "            loop.set_postfix(val_loss=(running_loss / total), val_acc=(accuracy / total))\n",
    "        return running_loss / total, accuracy / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use again the MNIST dataset. This time we will use the official train/test split!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We download the oficial MNIST train set\n",
    "all_train = datasets.MNIST('data/',\n",
    "                           download=True,\n",
    "                           train=True,\n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "# We split the whole train set in two parts:\n",
    "# the one that we actually use for training\n",
    "# and the one that we use for validation\n",
    "batch_size = 32 # size of the mini batch\n",
    "num_train = int(0.8 * len(all_train))\n",
    "\n",
    "trainset, valset = torch.utils.data.random_split(all_train, [num_train, len(all_train) - num_train])\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of the inputs are torch.Size([32, 1, 28, 28])\n",
      "The number on the image is: 2\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we can iterate over trainloader in the following way\n",
    "for inputs, targets in trainloader:\n",
    "    print(f'Dimensions of the inputs are {inputs.shape}')\n",
    "    plt.imshow(inputs[0][0], cmap='gray', interpolation='none')\n",
    "    print(f'The number on the image is: {targets[0]}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of ```inputs``` is $(32, 1, 28, 28)$. The first dimension indicates the size of the mini batch and is controlled by parameter ```batch_size```, the last two parameters are the 2D dimensions of the image and are equal to $28 \\times 28$ in case of the MNIST data. The lonely $1$, staying in the second dimension essentialy reflects the fact that the images are black and white. For instance, if MNIST were colored (there are variants of colored MNIST actually), then we would need $3$ (in case of RGB) colors to represent an image, thus $1$ would be replaed by $3$. \n",
    "\n",
    "**Question:** Run the above block several times. Is it plotting the same number all the time? If not, why?\n",
    "\n",
    "\n",
    "no trainloader with suffle = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Net + training parameters\n",
    "num_epochs = 2 # how many passes over the whole train data\n",
    "input_size = 784 # flattened size of the image\n",
    "hidden_sizes = [128, 64] # sizes of hidden layers\n",
    "output_size = 10 # how many labels we have\n",
    "lr = 0.001 # learning rate\n",
    "momentum = 0.9 # momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing our model/loss/optimizer\n",
    "net = SimpleFeedForward(input_size, hidden_sizes, output_size) # Our neural net\n",
    "criterion = nn.CrossEntropyLoss() # Loss function to be optimized\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr, momentum=momentum) # Optimization algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [1/2]: 100%|██████████| 1500/1500 [00:06<00:00, 249.98it/s]\n",
      "Validation in progress: 100%|██████████| 375/375 [00:01<00:00, 191.55it/s, val_acc=0.863, val_loss=0.52] \n",
      "Training Epoch [2/2]: 100%|██████████| 1500/1500 [00:06<00:00, 247.64it/s]\n",
      "Validation in progress: 100%|██████████| 375/375 [00:01<00:00, 202.05it/s, val_acc=0.894, val_loss=0.368]\n"
     ]
    }
   ],
   "source": [
    "# num_epochs indicates the number of passes over the data\n",
    "for epoch in range(num_epochs):    \n",
    "    # makes one pass over the train data and updates weights\n",
    "    train(net, trainloader, criterion, optimizer, epoch, num_epochs)\n",
    "    # makes one pass over validation data and provides validation statistics\n",
    "    val_loss, val_acc = validation(net, valloader, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation in progress: 100%|██████████| 313/313 [00:02<00:00, 147.22it/s, val_acc=0.9, val_loss=0.35]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.8998 | Test loss: 0.3497168891191483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Let us evaluate our net on the test set that we have never seen!\n",
    "testset = datasets.MNIST('data/',\n",
    "                         download=True,\n",
    "                         train=False,\n",
    "                         transform=transforms.ToTensor())\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_loss, test_acc = validation(net, testloader, criterion)\n",
    "print(f'Test accuracy: {test_acc} | Test loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Logistic regression via pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using above code as an example implement multinomial logistic regression and train it on the same data.\n",
    "For your report include:\n",
    "1. Mathematical description of logistic regression\n",
    "2. Mathematical description of optimization algorithm that you use\n",
    "3. High level idea of how to implement logisitic regression with pytorch\n",
    "4. Report classification accuracy on test data.\n",
    "\n",
    "**Solution**\n",
    "softmax ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multinomial logistic regression is used to model nominal outcome variables, in which the log odds of the outcomes are modeled as a linear combination of the predictor variables.\n",
    "\n",
    "The softmax function is simply a generalization of the logistic function that allows us to compute meaningful class-probabilities in multi-class settings (multinomial logistic regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Net + training parameters\n",
    "num_epochs = 3 # how many passes over the whole train data\n",
    "input_size = 784 # flattened size of the image\n",
    "output_size = 10 # how many labels we have\n",
    "lr = 0.001 # learning rate\n",
    "momentum = 0.9 # momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLogisticRegression(nn.Module):\n",
    "    def __init__(self, input_size=784,output_size=10):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(input_size, output_size)\n",
    "             \n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, input_size)\n",
    "        output = self.linear(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing our model/loss/optimizer\n",
    "model = MultiLogisticRegression(input_size, output_size) # Our neural net\n",
    "criterion = nn.CrossEntropyLoss() # This criterion combines nn.LogSoftmax() and nn.NLLLoss() in one single class.\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum) # Optimization algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [1/3]: 100%|██████████| 1500/1500 [00:04<00:00, 323.86it/s]\n",
      "Validation in progress: 100%|██████████| 375/375 [00:01<00:00, 214.76it/s, val_acc=0.867, val_loss=0.536]\n",
      "Training Epoch [2/3]: 100%|██████████| 1500/1500 [00:04<00:00, 329.71it/s]\n",
      "Validation in progress: 100%|██████████| 375/375 [00:01<00:00, 213.40it/s, val_acc=0.883, val_loss=0.446]\n",
      "Training Epoch [3/3]: 100%|██████████| 1500/1500 [00:04<00:00, 331.52it/s]\n",
      "Validation in progress: 100%|██████████| 375/375 [00:01<00:00, 193.17it/s, val_acc=0.891, val_loss=0.408]\n"
     ]
    }
   ],
   "source": [
    "# num_epochs indicates the number of passes over the data\n",
    "for epoch in range(num_epochs):    \n",
    "        # makes one pass over the train data and updates weights\n",
    "    train(model, trainloader, criterion, optimizer, epoch, num_epochs)\n",
    "        # makes one pass over validation data and provides validation statistics\n",
    "    val_loss, val_acc = validation(model, valloader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation in progress: 100%|██████████| 313/313 [00:01<00:00, 193.29it/s, val_acc=0.899, val_loss=0.383]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.8988 | Test loss: 0.3832653365135193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "testset = datasets.MNIST('data/',\n",
    "                         download=True,\n",
    "                         train=False,\n",
    "                         transform=transforms.ToTensor())\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "test_loss, test_acc = validation(model, testloader, criterion)\n",
    "print(f'Test accuracy: {test_acc} | Test loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elements of CNN: ```nn.Conv2d``` and ```MaxPool2d```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read this before starting: https://ttic.uchicago.edu/~shubhendu/Pages/Files/Lecture7_flat.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding the convolutional layer in pytorch**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we instanciate ```nn.Conv2d(1, 1, kernel_size=2, stride=[1, 1], padding=0)``` it has a parameter ```weight``` which precisely describes the kernel used for our convolution. In the beginning it is initialized randomly, and our goal is to eventually learn its weights (as usual via backpropagation!).\n",
    "Before building our first CNN let us have a look at the kernel and what it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 input channel (first 1 in nn.Conv2d)\n",
    "# 1 output channel (second 1 in nn.Conv2d)\n",
    "# 2x2 kernel (kernel_size=2)\n",
    "# the kernel slides by one step in (x, y) direction (stride=[1, 1])\n",
    "# we do not augment the picture with white borders (padding=0)\n",
    "conv = nn.Conv2d(1, 1, kernel_size=2, stride=[1, 1], padding=0) \n",
    "# Get kernel value.\n",
    "weight = conv.weight.data.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualization.** We will plot the initial image, the kernel, and the resulting image. In order to understand what is going on, the resulting image will be computed in two ways. First of all it will be computed by using ```conv1(image)```. Secondly, we will manually apply the sliding kernel to each $2\\times 2$ window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'By hand')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# take one image\n",
    "image, _ = next(iter(trainloader))\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 4)\n",
    "fig.tight_layout()\n",
    "fig.suptitle('Convolution')\n",
    "\n",
    "# plot the image\n",
    "axs[0].imshow(image[0][0], cmap='gray', interpolation='none')\n",
    "axs[0].set_title('Original image')\n",
    "\n",
    "# plot the kernel\n",
    "axs[1].imshow(weight[0][0], cmap='gray', interpolation='none')\n",
    "axs[1].set_title('2x2 kernel')\n",
    "\n",
    "# plot resulting image\n",
    "axs[2].imshow(conv(image)[0][0].detach().numpy(), cmap='gray', interpolation='none')\n",
    "axs[2].set_title('Resulting image')\n",
    "\n",
    "# Making the same by hands\n",
    "# IMPORTANT: we strongly suggest to understand the below code\n",
    "np_image = image[0][0].data.numpy() # get numpy image\n",
    "image_convolved = np.zeros((27, 27)) # here we store our result\n",
    "for i in range(27):\n",
    "    for j in range(27):\n",
    "        image_convolved[i, j] = np.sum(np_image[i:i+2, j:j+2] * weight) # apply the kernel for each 2x2 window\n",
    "        \n",
    "axs[3].imshow(image_convolved, cmap='gray', interpolation='none')\n",
    "axs[3].set_title('By hand')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem.** Provide 'by hand' implementation of the following kernel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, _ = next(iter(trainloader))\n",
    "image.size(2)\n",
    "\n",
    "pad = 0\n",
    "f = 4 #kernel is 4x4\n",
    "stride = 4\n",
    "n_output = int((image.size(2)-f +(2*pad))/stride +1 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'By hand')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1 input channel (first 1 in nn.Conv2d)\n",
    "# 1 output channel (second 1 in nn.Conv2d)\n",
    "# 4x4 kernel (kernel_size=4)\n",
    "# the kernel slides by 3 step in (x, y) direction (stride=[4, 4])\n",
    "# we do not augment the picture with white borders (padding=0)\n",
    "conv = nn.Conv2d(1, 1, kernel_size=4, stride=[4, 4], padding=0) \n",
    "# Get kernel value.\n",
    "weight = conv.weight.data.numpy()\n",
    "\n",
    "# take one image\n",
    "image, _ = next(iter(trainloader))\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 4)\n",
    "fig.tight_layout()\n",
    "fig.suptitle('Convolution')\n",
    "\n",
    "# plot the image\n",
    "axs[0].imshow(image[0][0], cmap='gray', interpolation='none')\n",
    "axs[0].set_title('Original image')\n",
    "\n",
    "# plot the kernel\n",
    "axs[1].imshow(weight[0][0], cmap='gray', interpolation='none')\n",
    "axs[1].set_title('4x4 kernel')\n",
    "\n",
    "# plot resulting image\n",
    "axs[2].imshow(conv(image)[0][0].detach().numpy(), cmap='gray', interpolation='none')\n",
    "axs[2].set_title('Resulting image')\n",
    "\n",
    "# Making the same by hands\n",
    "# PROBLEM: FILL IN THIS PART. \n",
    "np_image = image[0][0].data.numpy() # get numpy image\n",
    "image_convolved = np.zeros((27, 27)) # here we store our result\n",
    "\n",
    "# First specify size of the output : \n",
    "np_image = image[0][0].data.numpy() # get numpy image\n",
    "image_convolved = np.zeros((7, 7)) # here we store our result\n",
    "for i in range(0,27,4):\n",
    "    for j in range(0,27,4):\n",
    "        image_convolved[int(i/4), int(j/4)] = np.sum(np_image[i:i+4, j:j+4] * weight) # apply the kernel for each 4x4 window\n",
    "        \n",
    "axs[3].imshow(image_convolved, cmap='gray', interpolation='none')\n",
    "axs[3].set_title('By hand')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding the pooling layer in pytorch**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max pooling is what often used in practice, it amounts to picking only the largest value of a pixel in a given window. In pytorch it is done via ```MaxPool2d(kernel_size=k, stride=s)```, which has two parameters: kernel size and the stride. Note that there are no weights to learn here, so this layer is simply fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'By hand')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# kernel_size -- size of the max pool window\n",
    "pool = nn.MaxPool2d(kernel_size=4, stride=[4,4])\n",
    "\n",
    "fig, axs = plt.subplots(1, 3)\n",
    "fig.tight_layout()\n",
    "fig.suptitle('Pooling')\n",
    "\n",
    "# plot the image\n",
    "axs[0].imshow(image[0][0], cmap='gray', interpolation='none')\n",
    "axs[0].set_title('Original image')\n",
    "\n",
    "\n",
    "# plot resulting image\n",
    "axs[1].imshow(pool(image)[0][0].detach().numpy(), cmap='gray', interpolation='none')\n",
    "axs[1].set_title('Resulting image')\n",
    "\n",
    "# Making the same by hands\n",
    "# IMPORTANT: we strongly suggest to understand the below code\n",
    "np_image = image[0][0].data.numpy() # get numpy image\n",
    "image_pooled = np.zeros((7, 7)) # here we store our result\n",
    "for i in range(0, 27, 4):\n",
    "    for j in range(0, 27, 4):\n",
    "        image_pooled[int(i / 4), int(j / 4)] = np.max(np_image[i:i+4, j:j+4]) # max pooling\n",
    "        \n",
    "axs[2].imshow(image_pooled, cmap='gray', interpolation='none')\n",
    "axs[2].set_title('By hand')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a simple ConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=5, stride=[1, 1], padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(14 * 14 * 8, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, 10),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the first layer is ```nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2)```, the parameters here are chosen in such a way that the size of each output channel remains as $28 \\times 28$. Indeed, setting ```padding = 2``` we augmented our initial image to $32 \\times 32$, then we slide a kernel of size $5 \\times 5$ by $1$ in both $(x, y)$ directions which result in a $28 \\times 28$ output image (and $8$ channels).\n",
    "\n",
    "In general the formula for square images and squared kernels is\n",
    "$$\n",
    "    S_{out} = \\frac{S_{in} - S_{kernel} + 2S_{padding}}{S_{stride}} + 1\n",
    "$$\n",
    "\n",
    "In our case it is\n",
    "\n",
    "$$\n",
    "    S_{out} = \\frac{28 - 5 + 4}{1} + 1 = 28\n",
    "$$\n",
    "\n",
    "Then the output of ```nn.Conv2d(1, 8, kernel_size=5, stride=1, padding=2)``` goes into ```nn.ReLU()``` our favorite non-linearity and eventually into the pooling layer ```nn.MaxPool2d(kernel_size=2, stride=2)```.\n",
    "The ```nn.ReLU()``` doe not affect the size, hence ```nn.MaxPool2d(kernel_size=2, stride=2)``` receives $8$ channels of $28 \\times 28$ images as computed above.\n",
    "\n",
    "```nn.MaxPool2d(kernel_size=2, stride=2)``` will be applied to each single channel, with ```kernel_size=2, stride=2``` meaning that the output will still have $8$ channels but the images will be halfed in both $(x, y)$ directions. Hence the output of ```nn.MaxPool2d(kernel_size=2, stride=2)``` has $8$ channels with $14 \\times 14$ images.\n",
    "\n",
    "After all this, we will flatten our features and put the into simple ```nn.Linear(14 * 14 * 8, 500)```, where the input size is precisely the output size of ```nn.MaxPool2d(kernel_size=2, stride=2)```, and $500$ stands for the output size of this linear layer.\n",
    "Finally, we apply our favorite nonlinearity to ```nn.Linear(14 * 14 * 8, 500)``` followed by fully connected linear layer ```nn.Linear(500, 10)``` to match the dimension of $10$ classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [1/3]: 100%|██████████| 1500/1500 [00:17<00:00, 85.58it/s]\n",
      "Validation in progress: 100%|██████████| 375/375 [00:02<00:00, 136.10it/s, val_acc=0.972, val_loss=0.092] \n",
      "Training Epoch [2/3]: 100%|██████████| 1500/1500 [00:18<00:00, 82.44it/s]\n",
      "Validation in progress: 100%|██████████| 375/375 [00:02<00:00, 130.93it/s, val_acc=0.981, val_loss=0.0581]\n",
      "Training Epoch [3/3]: 100%|██████████| 1500/1500 [00:18<00:00, 82.27it/s]\n",
      "Validation in progress: 100%|██████████| 375/375 [00:03<00:00, 117.92it/s, val_acc=0.983, val_loss=0.0526]\n"
     ]
    }
   ],
   "source": [
    "net = ConvNet()\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # makes one pass over the train data and updates weights\n",
    "    train(net, trainloader, criterion, optimizer, epoch, num_epochs)\n",
    "\n",
    "    # makes one pass over validation data and provides validation statistics\n",
    "    val_loss, val_acc = validation(net, valloader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation in progress: 100%|██████████| 313/313 [00:02<00:00, 124.46it/s, val_acc=0.985, val_loss=0.0467]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9845 | Test loss: 0.046700128292664886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = validation(net, testloader, criterion)\n",
    "print(f'Test accuracy: {test_acc} | Test loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see the result here is much better, than in the simple multilayer perceptron. But note, we have actualy trained muuuuuch more parameters here and, at least on my computer, it takes considerably more time.\n",
    "\n",
    "Here you can see the summary of current state of the art results on MNIST: https://www.kaggle.com/c/digit-recognizer/discussion/61480\n",
    "\n",
    "As you see our score barely beats a carefully built random forest or **kNN**! To get extra $0.01$ requires much more fine tuning, which is of course is not the goal here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the code for ConvNet and insert Dropout layer (whereever you want).\n",
    "\n",
    "Include in your report:\n",
    "1. High level description of the dropout\n",
    "2. High level description of your architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of dropout : \n",
    "\n",
    "Dropout is a regularization method that mimics the training a large number of neural networks with different architectures in parallel. It has the property to reduce variance. The more complex our neural net, the more it has the opportunity to overfit the training data. \n",
    "The dropout technique is the often used to reduce overfitting in large neural networks. \n",
    "\n",
    "During training, some number of nodes in a layer outputs are randomly ignored (aka  “dropped out.”). We choose the probability of an element to be zeroed as a hyperparameter. This has the effect of making the layer look-like a layer with different number of nodes . In effect, each update to a layer during training is performed with a different “architecture” of the configured layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description of architecture \n",
    "We modified the previous```ConvNet```, we set a dropout layer right after layer1 :```nn.Conv2d(1, 8, kernel_size=5, stride=1, padding=2)``` (208 parameters), ```nn.ReLU()``` and ```nn.MaxPool2d(kernel_size=2, stride=2)```.\n",
    "\n",
    "The ```nn.ReLU()``` doe not affect the size, hence ```nn.MaxPool2d(kernel_size=2, stride=2)``` receives $8$ channels of $28 \\times 28$ images as computed above.\n",
    "\n",
    "The output of the ```nn.MaxPool2d``` layer is then a $14 \\times 14$ images. with $8$ channels.\n",
    "\n",
    "We apply a dropout Layer to the flattened image a $1580$ vector with probability of a neuron to be dropped of $0.25$\n",
    "\n",
    "\n",
    "We then have a fully connected layer of ```nn.Linear(1580, 500)``` followed by ```nn.ReLU()``` ( $784 500$ parameters).\n",
    "\n",
    "And the last layer consisting of a Fully connected```nn.Linear(500, 10)``` ($5010$ parameters) to match the output 10 classes dimension\n",
    "\n",
    "The total number of parameters to train is  : $789718$.\n",
    "\n",
    "Parameter size is : $3.01$ MB \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=5, stride=[1, 1], padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(14 * 14 * 8, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, 10),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(0.30)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        #Apply dropout\n",
    "        out = self.dropout(out)\n",
    "        out = self.classifier(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [1/3]: 100%|██████████| 1500/1500 [00:17<00:00, 87.76it/s]\n",
      "Validation in progress: 100%|██████████| 375/375 [00:02<00:00, 137.40it/s, val_acc=0.972, val_loss=0.0863]\n",
      "Training Epoch [2/3]: 100%|██████████| 1500/1500 [00:17<00:00, 85.01it/s]\n",
      "Validation in progress: 100%|██████████| 375/375 [00:02<00:00, 131.66it/s, val_acc=0.981, val_loss=0.0607]\n",
      "Training Epoch [3/3]: 100%|██████████| 1500/1500 [00:19<00:00, 76.24it/s]\n",
      "Validation in progress: 100%|██████████| 375/375 [00:03<00:00, 122.75it/s, val_acc=0.985, val_loss=0.0509]\n"
     ]
    }
   ],
   "source": [
    "net = ConvNet()\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # makes one pass over the train data and updates weights\n",
    "    train(net, trainloader, criterion, optimizer, epoch, num_epochs)\n",
    "\n",
    "    # makes one pass over validation data and provides validation statistics\n",
    "    val_loss, val_acc = validation(net, valloader, criterion)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation in progress: 100%|██████████| 313/313 [00:02<00:00, 114.17it/s, val_acc=0.985, val_loss=0.0476]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9851 | Test loss: 0.04762276781257242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = validation(net, testloader, criterion)\n",
    "print(f'Test accuracy: {test_acc} | Test loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 8, 28, 28]             208\n",
      "              ReLU-2            [-1, 8, 28, 28]               0\n",
      "         MaxPool2d-3            [-1, 8, 14, 14]               0\n",
      "           Dropout-4                 [-1, 1568]               0\n",
      "            Linear-5                  [-1, 500]         784,500\n",
      "              ReLU-6                  [-1, 500]               0\n",
      "            Linear-7                   [-1, 10]           5,010\n",
      "================================================================\n",
      "Total params: 789,718\n",
      "Trainable params: 789,718\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.13\n",
      "Params size (MB): 3.01\n",
      "Estimated Total Size (MB): 3.14\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(net,(1,28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the succesful completion of this TP, we expect you to be able to understand the architectures of NN, CNN.\n",
    "For instance, have a look at the famous AlexNet https://github.com/pytorch/vision/blob/master/torchvision/models/alexnet.py and see if you can understand its architechture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
